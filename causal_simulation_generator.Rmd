---
title: "Graphical Simulator Generator"
author: "Bob and Associates"
date: "2023-05-02"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a framework to generate simulated data given a causal graph and some evaluation criteria. We use it to implement Judea Pearl's "Multistage Simpson's Paradox machine" which is described in Figure 3 of this article on [Understanding Simpson's Paradox](http://ftp.cs.ucla.edu/pub/stat_ser/r414-reprint.pdf)".


```{r libraries, warning=FALSE}
library(dplyr)
library(ggplot2)
library(bnlearn)  # model2network
library(Rgraphviz)
```

(The plot of the graph is the only thing that depends on Rgraphviz, which needs to be installed from Bioconductor, or bnlearn which also has some complicated dependencies as I recall. You don't need these packages to run the simulation.)

# Berkson's Paradox

Define the causal graph and display it:

```{r msspm_graph}

msspm_str <-"[Z1            ]
             [Z3 | Z1       ]
             [Z3b| Z1       ]
             [Z2 | Z3  : Z3b]
             [Z5 | Z3       ]
             [Z5b| Z3b      ]
             [Z4 | Z5  : Z5b]
             [X  | Z5b      ]
             [Y  | X   : Z5 ]"  %>% gsub('\\s', '', .)

msspm_str %>% model2network %>% graphviz.plot


```

Parse the graph to list the parents of each node.

```{r parse_graph_str}

parse_graph_str <- function(graph_str){
  parts <- graph_str %>%
    strsplit('\\]\\[') %>% '[['(1) %>% 
    gsub('[', '', ., fixed=TRUE) %>% 
    gsub(']', '', ., fixed=TRUE) %>% 
    strsplit('[\\|:] ?')
  
  setNames(parts %>% sapply('[', -1), nm=parts %>% sapply('[', 1))
}

node_parents <- parse_graph_str(msspm_str)

node_parents

```



Simulate data according to the dependency relationships in the graph, basically as follows: First find the nodes with no parents and give them random values. Then find the nodes whose parents have already been simulated, and give them values using a function that depends on the parents (here we use simple linear functions, but other functions should be possible). Repeat until all nodes have been added as columns in the dataframe of simulated data.

But the graph only tells us which variables depend on which other nodes; it does not tell us their weights. Basically, every edge in the graph can have a weight, and we can pass the set of edge weights into the generation function as parameters. We need to do some gymnastics to get these parameters into a one-dimensional vector, since that is what the optimizer expects. We just put the edges in order alphabetically by the origin and destination.

We also need a loss function to optimize. Since out goal is to make a dataset where including specific sets of covariates gives particular coefficiennts for `X`, we write a loss function to measure how close we come to achieving that goal.


```{r sim_data}

deserialize_betas <- function(par, node_parents){
  if ( length(par) != length(unlist(node_parents)))
    print("Error: length of parameter list does not match length of node parents data structrure.")
  
  i <- 1
  beta_lookup = list()
  for (node_name in sort(names(node_parents))){
    ## print(sprintf("node name: %s", node_name))
    parents <- node_parents[[node_name]]
    
    if (length(parents) > 0){ # skip nodes without parents
      for (parent_name in sort(parents)){
        ## print(sprintf("    parent name: %s", parent_name))
        beta_lookup[[node_name]][parent_name] <- par[i]
        i <- i + 1
      }
    }

  }
  
  beta_lookup
}


simulation_function <- function(data_cols, beta_vec, noise=0.01){
  M <- data_cols %>% as.data.frame %>% as.matrix
  noise <- rnorm(nrow(M), sd=noise)
  M %*% beta_vec + noise
}


simulate_with_pars <- function(par, node_parents, num_rows=1000, my_noise=0.1){

  betas <- deserialize_betas(par, node_parents)
  
  columns <- list()
  
  remaining_nodes <- node_parents
  
  
  while (length(remaining_nodes) > 0){
    for (node_name in names(remaining_nodes)){
      my_parents <- remaining_nodes[[node_name]]
      if (length(my_parents) == 0){
        # no dependencies
  	    ## print(sprintf("Adding root node %s", node_name))
        columns[[node_name]] = runif(num_rows)
        remaining_nodes[[node_name]] <- NULL
      } else if ( length(setdiff(my_parents, names(columns))) == 0 ) {
        # all dependencies are already in the dataset
  	    ## print(sprintf("Adding dependent node %s", node_name))
        data_cols <- columns[unlist(my_parents)]
        beta_vec <- betas[[node_name]][names(data_cols)]
        columns[[node_name]] = simulation_function(data_cols, beta_vec, my_noise)
  	    remaining_nodes[node_name] <- NULL
      } else {
        print(sprintf("Starting over for node %s", node_name))
  	  }
    }
  }
  
  as.data.frame(columns)
}


get_coef_vector_for_dataframe <- function(df){
  c(
    'just_X' = coef( lm(Y ~ X, df) )['X'],
    'add_Z1' = coef( lm(Y ~ X + Z1, df) )['X'],
    'add_Z2' = coef( lm(Y ~ X + Z1 + Z2, df) )['X'],
    'add_Z3' = coef( lm(Y ~ X + Z1 + Z2 + Z3, df) )['X'],
    'add_Z4' = coef( lm(Y ~ X + Z1 + Z2 + Z3 + Z4, df) )['X'],
    'add_Z5' = coef( lm(Y ~ X + Z1 + Z2 + Z3 + Z4 + Z5, df) )['X']
  )
}


v_score <- function(coef_vector){
  # This is basically Root Mean Squared Error with an extra penalty for getting the signs wrong.
  target_vector <- c(1, -1,  1, -1,  1, -1 ) #? c(1, -2,  3, -4,  5, -6 )
  sign_penalty <- sum(10 * (sign(coef_vector) != sign(target_vector)))
  rmse <- sqrt(mean((coef_vector - target_vector)^2))
  sign_penalty + rmse
}


score_parameters <- function(par, ...){
  
  df <- simulate_with_pars(par, node_parents, ...)
  
  coef_vector <- get_coef_vector_for_dataframe(df)
  
  v_score(coef_vector)
}

```

Find the optimal parameters using R's built-in optimizeer:

```{r optimize_parameters}

set.seed(42)

par <- runif(11, min=-0.1, max=0.1)

results <- optim(par, score_parameters, method='SANN', 
                 control=list(maxit=5000))

results$par %>% simulate_with_pars(node_parents, num_rows=1000) %>% get_coef_vector_for_dataframe


```

This is what the resulting dataframe looks like:

```{r examine_df}
mssp <- simulate_with_pars(results$par, node_parents, num_rows=1000, my_noise=0.1)
write.csv(mssp, "multi_stage_simpsons_paradox_data.csv")

mssp %>% head
```

Now we can test whether the reversals we are looking for in the coefficient of 'X' actually happen. If the confounding is correctly implemented, we expect to see the sign of the coefficient of 'X' flip back and forth between negative and positive as we add the covariates to the analysis in order.

```{r test_for_reversals}

coef( lm(Y ~ X, mssp) )['X']
coef( lm(Y ~ X + Z1, mssp) )['X']
coef( lm(Y ~ X + Z1 + Z2, mssp) )['X']
coef( lm(Y ~ X + Z1 + Z2 + Z3, mssp) )['X']
coef( lm(Y ~ X + Z1 + Z2 + Z3 + Z4, mssp) )['X']
coef( lm(Y ~ X + Z1 + Z2 + Z3 + Z4 + Z5, mssp) )['X']

```


